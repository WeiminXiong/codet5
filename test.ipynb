{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from human_eval.data import write_jsonl, read_problems, stream_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(prompt, remove_lines=True):\n",
    "    token = '\\\"\\\"\\\"'\n",
    "    start = token\n",
    "    end = '>>>'\n",
    "\n",
    "    start_idx = prompt.find(start) + len(start)\n",
    "    end_idx = prompt.find(end)\n",
    "\n",
    "    output = prompt[start_idx: end_idx]\n",
    "    if remove_lines:\n",
    "        output = output.replace('\\n', ' ')\n",
    "    output = re.sub(r\"\\s+\", \" \", output).strip()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "INSTRUCTION = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "Create a Python script for this problem:\n",
    "{}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 200,\n",
      " 'decoding_style': 'sampling',\n",
      " 'end_index': 164,\n",
      " 'max_len': 800,\n",
      " 'model': 'Salesforce/codet5p-2b',\n",
      " 'num_seqs_per_iter': 4,\n",
      " 'output_path': None,\n",
      " 'overwrite': False,\n",
      " 'start_index': 0,\n",
      " 'temperature': 0.2}\n",
      "Number of samples: 164\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model', type=str, default='Salesforce/codet5p-2b', help=\"\")\n",
    "parser.add_argument('--output_path', type=str, help=\"\")\n",
    "parser.add_argument('--start_index', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--end_index', type=int, default=164, help=\"\")\n",
    "parser.add_argument('--temperature', type=float, default=0.2, help=\"\")\n",
    "parser.add_argument('--N', type=int, default=200, help=\"\")\n",
    "parser.add_argument('--max_len', type=int, default=800, help=\"\")\n",
    "parser.add_argument('--decoding_style', type=str, default='sampling', help=\"\")\n",
    "parser.add_argument('--num_seqs_per_iter', type=int, default=4, help='')\n",
    "parser.add_argument('--overwrite', action='store_true', help='')\n",
    "\n",
    "args,_ = parser.parse_known_args()\n",
    "\n",
    "argsdict = vars(args)\n",
    "print(pprint.pformat(argsdict))\n",
    "\n",
    "STOP_SEQS = ['\\nclass', '\\ndef', '\\n#', '\\nif', '\\nprint']\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "problems = read_problems()\n",
    "\n",
    "task_ids = sorted(problems.keys())[args.start_index: args.end_index]\n",
    "prompts = [problems[task_id]['prompt'] for task_id in task_ids]\n",
    "num_samples = len(prompts)\n",
    "print(\"Number of samples: {}\".format(num_samples))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model,\n",
    "                                                trust_remote_code=True,  # False for 220m and 770m models\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                low_cpu_mem_usage=True\n",
    "                                                )\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# for larger LLMs such as 2B, 6B, and 16B, we need to pass the text prompt to the decoder\n",
    "prompt_to_decoder = True if any([size in args.model for size in ['2b', '6b', '16b']]) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"def fizz_buzz(n: int):\\n    \\\"\\\"\\\">>> fizz_buzz(50)\\n    0\\n    >>> fizz_buzz(78)\\n    2\\n    >>> fizz_buzz(79)\\n    3\\n    \\\"\\\"\\\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1196462/4001530116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprompt_batch_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m encoding_decoder = tokenizer(prompt_batch_decoder, return_tensors=\"pt\", truncation=True,\n\u001b[1;32m      5\u001b[0m                                      max_length=args.max_len).to(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_batch = [prompt]\n",
    "encoding = tokenizer(prompt_batch, return_tensors=\"pt\", truncation=True, max_length=args.max_len).to(device)\n",
    "prompt_batch_decoder = [prompt]\n",
    "encoding_decoder = tokenizer(prompt_batch_decoder, return_tensors=\"pt\", truncation=True,\n",
    "                                     max_length=args.max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(**encoding,\n",
    "                            decoder_input_ids=encoding_decoder['input_ids'],\n",
    "                            do_sample=True,\n",
    "                            temperature=0.2,\n",
    "                            max_length=args.max_len,\n",
    "                            num_return_sequences=2,\n",
    "                            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_seqs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fizz_buzz(n: int):\n",
      "    \"\"\"return the number of times the number 7 appears in integers less than n which are divisible by 11 or 13\n",
      "    >>> fizz_buzz(50)\n",
      "    0\n",
      "    >>> fizz_buzz(78)\n",
      "    2\n",
      "    >>> fizz_buzz(79)\n",
      "    3\n",
      "    \"\"\"\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        if i % 11 == 0 or i % 13 == 0:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "\n",
      "    doctest.testmod()\n",
      "    print(fizz_buzz(50))\n",
      "    print(fizz_buzz(78))\n",
      "    print(fizz_buzz(79))\n",
      "    print(fizz_buzz(100))\n",
      "    print(fizz_buzz(101))\n",
      "    print(fizz_buzz(200))\n",
      "    print(fizz_buzz(201))\n",
      "    print(fizz_buzz(300))\n",
      "    print(fizz_buzz(301))\n",
      "    print(fizz_buzz(400))\n",
      "    print(fizz_buzz(401))\n",
      "    print(fizz_buzz(500))\n",
      "    print(fizz_buzz(501))\n",
      "    print(fizz_buzz(600))\n",
      "    print(fizz_buzz(601))\n",
      "    print(fizz_buzz(700))\n",
      "    print(fizz_buzz(701))\n",
      "    print(fizz_buzz(800))\n",
      "    print(fizz_buzz(801))\n",
      "    print(fizz_buzz(900))\n",
      "    print(fizz_buzz(901))\n",
      "    print(fizz_buzz(1000))\n",
      "    print(fizz_buzz(1001))\n",
      "    print(fizz_buzz(2000))\n",
      "    print(fizz_buzz(2001))\n",
      "    print(fizz_buzz(3000))\n",
      "    print(fizz_buzz(3001))\n",
      "    print(fizz_buzz(4000))\n",
      "    print(fizz_buzz(4001))\n",
      "    print(fizz_buzz(5000))\n",
      "    print(fizz_buzz(5001))\n",
      "    print(fizz_buzz(6000))\n",
      "    print(fizz_buzz(6001))\n",
      "    print(fizz_buzz(7000))\n",
      "    print(fizz_buzz(7001))\n",
      "    print(fizz_buzz(8000))\n",
      "    print(fizz_buzz(8001))\n",
      "    print(fizz_buzz(9000))\n",
      "    print(fizz_buzz(9001))\n",
      "    print(fizz_buzz(10000))\n",
      "    print(fizz_buzz(10001))\n",
      "    print(fizz_buzz(20000))\n",
      "    print(fizz_buzz(20001))\n",
      "    print(fizz_buzz(30000))\n",
      "    print(fizz_buzz(30001))\n",
      "    print(fizz_buzz(40000))\n",
      "    print(fizz_buzz(40001))\n",
      "    print(fizz_buzz(50000))\n",
      "    print(fizz_buzz(50001))\n",
      "    print(fizz_buzz(60000))\n",
      "    print(fizz_buzz(60001))\n",
      "    print(fizz_buzz(70000))\n",
      "    print(fizz_buzz\n",
      "def fizz_buzz(n: int):\n",
      "    \"\"\"return the number of times the number 7 appears in integers less than n which are divisible by 11 or 13\n",
      "    >>> fizz_buzz(50)\n",
      "    0\n",
      "    >>> fizz_buzz(78)\n",
      "    2\n",
      "    >>> fizz_buzz(79)\n",
      "    3\n",
      "    \"\"\"\n",
      "    count = 0\n",
      "    for i in range(1, n):\n",
      "        if i % 11 == 0 or i % 13 == 0:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "    doctest.testmod()\n",
      "    print(fizz_buzz(50))\n",
      "    print(fizz_buzz(78))\n",
      "    print(fizz_buzz(79))\n",
      "    print(fizz_buzz(100))\n",
      "    print(fizz_buzz(101))\n",
      "    print(fizz_buzz(200))\n",
      "    print(fizz_buzz(201))\n",
      "    print(fizz_buzz(300))\n",
      "    print(fizz_buzz(301))\n",
      "    print(fizz_buzz(400))\n",
      "    print(fizz_buzz(401))\n",
      "    print(fizz_buzz(500))\n",
      "    print(fizz_buzz(501))\n",
      "    print(fizz_buzz(600))\n",
      "    print(fizz_buzz(601))\n",
      "    print(fizz_buzz(700))\n",
      "    print(fizz_buzz(701))\n",
      "    print(fizz_buzz(800))\n",
      "    print(fizz_buzz(801))\n",
      "    print(fizz_buzz(900))\n",
      "    print(fizz_buzz(901))\n",
      "    print(fizz_buzz(1000))\n",
      "    print(fizz_buzz(1001))\n",
      "    print(fizz_buzz(2000))\n",
      "    print(fizz_buzz(2001))\n",
      "    print(fizz_buzz(3000))\n",
      "    print(fizz_buzz(3001))\n",
      "    print(fizz_buzz(4000))\n",
      "    print(fizz_buzz(4001))\n",
      "    print(fizz_buzz(5000))\n",
      "    print(fizz_buzz(5001))\n",
      "    print(fizz_buzz(6000))\n",
      "    print(fizz_buzz(6001))\n",
      "    print(fizz_buzz(7000))\n",
      "    print(fizz_buzz(7001))\n",
      "    print(fizz_buzz(8000))\n",
      "    print(fizz_buzz(8001))\n",
      "    print(fizz_buzz(9000))\n",
      "    print(fizz_buzz(9001))\n",
      "    print(fizz_buzz(10000))\n",
      "    print(fizz_buzz(10001))\n",
      "    print(fizz_buzz(20000))\n",
      "    print(fizz_buzz(20001))\n",
      "    print(fizz_buzz(30000))\n",
      "    print(fizz_buzz(30001))\n",
      "    print(fizz_buzz(40000))\n",
      "    print(fizz_buzz(40001))\n",
      "    print(fizz_buzz(50000))\n",
      "    print(fizz_buzz(50001))\n",
      "    print(fizz_buzz(60000))\n",
      "    print(fizz_buzz(60001))\n",
      "    print(fizz_buzz(70000))\n",
      "    print(fizz_\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(gen_seqs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
